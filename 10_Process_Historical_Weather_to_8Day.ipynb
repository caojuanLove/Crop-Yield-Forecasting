{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf754451-2b53-478a-9d67-fc3c5623fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将预报数据和S2S都重组成LSTM模型需要的每年的数据，并输出到04_mergeData，包含两个文件夹\n",
    "# 01_ECMWF;02_histdata[mean和similar]\n",
    "\n",
    "def process_climate_data(data_new, year, T_upper, T_lower, dynamic_features):\n",
    "    # 选择列\n",
    "    Tmin_columns = [col for col in data_new.columns if '_Tmin' in col]\n",
    "    Tmin = data_new[Tmin_columns].values\n",
    "    Tmean_columns = [col for col in data_new.columns if '_Tmean' in col]\n",
    "    Tmean = data_new[Tmean_columns].values\n",
    "    Tmax_columns = [col for col in data_new.columns if '_Tmax' in col]\n",
    "    Tmax = data_new[Tmax_columns].values\n",
    "    Pre_columns = [col for col in data_new.columns if '_Pre' in col]\n",
    "    Pre = data_new[Pre_columns].values\n",
    "    \n",
    "    # 计算日期范围\n",
    "    days = Pre.shape[1]\n",
    "    dates = pd.date_range(start=str(year) + '-01-01', periods=days, freq='D')\n",
    "    \n",
    "    # 添加年份信息\n",
    "    data_new['year'] = year\n",
    "    \n",
    "    # 计算极端气象指标\n",
    "    spei_df = spei(dates, Pre, Tmean)\n",
    "    CDD_df, HDD_df, GDD_df = extreme_temperature(dates, Tmax, Tmin, T_upper, T_lower)\n",
    "    \n",
    "    # 聚合8天的数据\n",
    "    data_new1 = aggre_8days(dynamic_features, dates, data_new)\n",
    "    \n",
    "    # 合并所有数据\n",
    "    data_new1 = pd.concat([CDD_df, HDD_df, GDD_df, spei_df, data_new1], axis=1)\n",
    "    \n",
    "    return data_new1\n",
    "    \n",
    "def find_weeks(forecastDataList, week_dates):\n",
    "    result = []\n",
    "    # 遍历 forecastDataList 中的每个日期\n",
    "    for date in forecastDataList:\n",
    "        # 遍历 week_dates，以便找到日期所在的 week\n",
    "        for i in range(len(week_dates) - 1):\n",
    "            # 检查日期是否在当前日期范围内（包括下边界但不包括上边界）\n",
    "            if week_dates[i] <= date < week_dates[i + 1]:\n",
    "                result.append((date, i + 1))  # week 1 对应的 index 是 0，所以 week 是 i + 1\n",
    "                break\n",
    "        # 如果日期是最后一个日期范围之外的情况（即 week46 的范围）\n",
    "        else:\n",
    "            if date >= week_dates[-1]:\n",
    "                result.append((date, len(week_dates)))  # 最后一周 week46\n",
    "    result = {date: week for date, week in result}\n",
    "    return result\n",
    "\n",
    "def update_S2Sandhist_VI(data_S2S_new_all_new, VI_select2, result, years, start_point, harvest_point, outpath_S2S,ii,type):\n",
    "    # 设置索引\n",
    "  #  data_S2S_new_all_new.set_index(['year', 'idJoin'], inplace=True)\n",
    "\n",
    "    # 筛选包含 VI_select2 的列\n",
    "    filtered_columns = [col for col in data_S2S_new_all_new.columns if VI_select2 in col]\n",
    "    data_S2S_VI = data_S2S_new_all_new[filtered_columns].reset_index()\n",
    "\n",
    "    # 初始化更新的 DataFrame\n",
    "    update_VI = pd.DataFrame()\n",
    "\n",
    "    # 逐年更新数据\n",
    "    for year in years:\n",
    "        week_forecast = result[ii]\n",
    "        forecast_weeklist = range(week_forecast, harvest_point + 1)\n",
    "        actual_weeklist = range(start_point, week_forecast)\n",
    "        \n",
    "        forecast_weeklist = [f'Week{week}{VI_select2}' for week in forecast_weeklist]\n",
    "        before_weeklist = [f'Week{week}{VI_select2}' for week in actual_weeklist]\n",
    "\n",
    "        # 计算当前年的历史均值和预测均值\n",
    "        data_S2S_VI_before = data_S2S_VI[before_weeklist + ['year']].groupby('year').mean()\n",
    "        data_S2S_VI_forecast = data_S2S_VI[forecast_weeklist + ['year']].groupby('year').mean()\n",
    "        \n",
    "        # 提取当前年的数据\n",
    "        current_S2S_VI = data_S2S_VI[data_S2S_VI['year'] == year]\n",
    "\n",
    "        # 计算 DTW 距离\n",
    "        dtw_distances = {}\n",
    "        for year1 in years:\n",
    "            current_S2S_VI_before = data_S2S_VI_before.loc[year]\n",
    "            if year1 < year: # 只从前面的年份进行预报\n",
    "                other_S2S_VI_before = data_S2S_VI_before.loc[year1]\n",
    "                distance, path = fastdtw(current_S2S_VI_before, other_S2S_VI_before)\n",
    "                dtw_distances[year1] = distance\n",
    "\n",
    "        # 找到最相似的年份\n",
    "        most_similar_by_dtw = min(dtw_distances, key=dtw_distances.get)\n",
    "        dataVI_similaryear = data_S2S_VI[data_S2S_VI['year'] == most_similar_by_dtw]\n",
    "\n",
    "        # 更新当前年的预测周数据\n",
    "        current_S2S_VI[forecast_weeklist] = dataVI_similaryear[forecast_weeklist].values\n",
    "        current_S2S_VI['year'] = year\n",
    "        current_S2S_VI['idJoin'] = dataVI_similaryear['idJoin']\n",
    "\n",
    "        # 合并更新的数据\n",
    "        update_VI = pd.concat([update_VI, current_S2S_VI], axis=0)\n",
    "\n",
    "    # 将更新后的数据重新设置索引\n",
    "    update_VI.set_index(['year', 'idJoin'], inplace=True)\n",
    "    data_S2S_new_all_new[forecast_weeklist] = update_VI[forecast_weeklist].values\n",
    "\n",
    "    # 保存结果\n",
    "    output_path = os.path.join(outpath_S2S, 'data_'+type+'.csv')\n",
    "    data_S2S_new_all_new.to_csv(output_path)\n",
    "    print(f\"Updated data saved to {output_path}\")\n",
    "\n",
    "    return data_S2S_new_all_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d54a88-9c0e-46b1-995d-09f06242b0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: F:\\SCI\\SCI9_1\\01_code\\02_Wheat\\06_India\n",
      "当前文件夹名字: 06_India\n",
      "上一级文件夹名字: 02_Wheat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "root_directory = os.getcwd()[0:3]\n",
    "sys.path.append(root_directory+'\\\\SCI\\\\SCI9_1\\\\01_code')\n",
    "sys.path.append(r'C:\\ProgramData\\anaconda3\\Lib\\site-packages') \n",
    "sys.path.append(r'C:\\Users\\DELL\\.conda\\envs\\myenv\\Lib\\site-packages') \n",
    "sys.path.append(r'C:\\Users\\DELL\\.conda\\envs\\rasterio_env\\Lib\\site-packages') \n",
    "from functions import spei,extreme_temperature,aggre_8days,extract_dates\n",
    "from sklearn.metrics import mean_absolute_percentage_error, accuracy_score, roc_auc_score, roc_curve,r2_score,mean_squared_error\n",
    "from functions import calculate_rrmse1,calculate_rrmse2,calculate_acc,calculate_nrmse,calculate_mare,extract_selected_variables\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "\n",
    "VIs =  ['_KNDVI' ,'_EVI','_NDVI']\n",
    "Cilmate = ['_Pre' ,'_Tmin' ,'_Solar','_Tmean','_Tmax']\n",
    "Climate_Exogenous  = ['_CDD' ,'_HDD' ,'_GDD','_VPD','_wind_speed','_SPEI'] #'_VPD','_wind_speed',\n",
    "soil_feature = [ 'SAND','AWC', 'SILT','ORG_CARBON',  'TOTAL_N', 'PH_WATER',  'CEC_SOIL', 'CLAY']\n",
    "loc_feature = ['elevation', 'lat', 'lon']\n",
    "Year_feature = ['year'];union_feature = ['idJoin'];\n",
    "dynamic_features = [ '_KNDVI' ,'_EVI','_NDVI','_Pre' ,'_Tmin' ,'_Solar','_Tmean','_VPD', '_wind_speed' ,'_Tmax']\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ast\n",
    "from fastdtw import fastdtw\n",
    " \n",
    "# 获取当前工作目录\n",
    "current_directory = os.getcwd()\n",
    "print(\"当前工作目录:\", current_directory)\n",
    " \n",
    "# 获取当前文件夹的名字\n",
    "current_folder_name = os.path.basename(current_directory)\n",
    "print(\"当前文件夹名字:\", current_folder_name)\n",
    " \n",
    "# 获取上一级文件夹的名字\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "parent_folder_name = os.path.basename(parent_directory)\n",
    "print(\"上一级文件夹名字:\", parent_folder_name)\n",
    "\n",
    "crop = parent_folder_name;countryID =current_folder_name\n",
    "# 需要改变的变量\n",
    "country = countryID.split('_')[1]\n",
    "##############地区区域#############################################\n",
    "inpath_dates_other = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID+'\\\\'+'01_data'+'\\\\'+'07_Information'\n",
    "other_infornamtion = pd.read_csv(os.path.join(inpath_dates_other,'information.txt'), sep=' ', header=None)\n",
    "startyear,endyear,shp_name = other_infornamtion.iloc[0,0],other_infornamtion.iloc[0,1],other_infornamtion.iloc[0,2]\n",
    "\n",
    "inputpath_base = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID\n",
    "\n",
    "\n",
    "Forecastyear = endyear\n",
    "\n",
    "\n",
    "years = range(startyear,endyear+1)\n",
    "regions = ['I']#\n",
    "Forecastyears = {\n",
    "    'I': endyear, \n",
    "}\n",
    "# 按照作物定义温度阈值\n",
    "if crop == '02_Wheat':\n",
    "    T_upper = 34\n",
    "    T_lower = 0\n",
    "elif crop == '01_Maize':  # 修正了拼写错误\n",
    "    T_upper = 30\n",
    "    T_lower = 8\n",
    "elif crop == '03_Rice':\n",
    "    T_upper = 35\n",
    "    T_lower = 8    \n",
    "else:\n",
    "    T_upper = 30\n",
    "    T_lower = 10\n",
    "\n",
    "\n",
    "\n",
    "inputpath_base = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID+'\\\\'\n",
    "institution = 'ECMWF';ECMWF_path = os.path.join(inputpath_base,'02_S2S')\n",
    "\n",
    "file_path = os.path.join(inputpath_base, '02_S2S', '01_dataori', 'ECMWF','CommonYear_Week.txt')\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = [line.strip() for line in file.readlines()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8892e811-2d2b-4bfa-8cd1-c50fa24f38c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70590a93-7268-4b6f-a8ea-8209a67de9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "【20250101】\n",
    "# 1、# 历史数据;将其表头修改掉\n",
    "【20250106】\n",
    "# 1、重新修改了start_point<harvest_point的存在一小部分问题，主要是before数据替换的不太正确\n",
    "# 2、添加了start_point>harvest_point，即跨年的情况\n",
    "'''\n",
    "\n",
    "\n",
    "for region in regions:\n",
    "    Forecastyear = Forecastyears[region]\n",
    "    hist_outputpath = os.path.join(inputpath_base,'02_S2S','03_outputData','02_histdata',region)\n",
    "    os.makedirs(hist_outputpath,exist_ok=True)\n",
    "    pre_name = 'Wheat_'+region+'_';\n",
    "    hist_inputpath = os.path.join(inputpath_base,'01_data','04_GEEdownloadData','02_histdata')\n",
    "    data = pd.read_csv(os.path.join(hist_inputpath,pre_name+str(1990)+'.csv'));\n",
    "    data.columns = data.columns.str.replace(rf'^{1990}', '', regex=True)\n",
    "    columns_sta = data.columns\n",
    "    hist_start_year = Forecastyear-31;hist_end_year = Forecastyear-1;\n",
    "    allhist = pd.DataFrame()\n",
    "    \n",
    "    for year_hist in range(hist_start_year,hist_end_year+1):\n",
    "        data = pd.read_csv( os.path.join(hist_inputpath,pre_name+str(year_hist)+'.csv'));\n",
    "        data.columns = data.columns.str.replace(rf'^{year_hist}', '', regex=True)\n",
    "        data = data[columns_sta]\n",
    "        data['idGroup'] = data['idJoin']\n",
    "        data.drop(['idGroup', 'iso3', '.geo','system:index'], axis=1, inplace=True)#'idGroup', \n",
    "        # data.drop(['idGroup', 'iso3', 'lat', 'lon', '.geo','system:index'], axis=1, inplace=True)#'idGroup', \n",
    "        data.columns = str(Forecastyear) + data.columns\n",
    "        data.rename(columns={f\"{Forecastyear}idJoin\": \"idJoin\"}, inplace=True)\n",
    "        data.to_csv(os.path.join(hist_outputpath,'hist_'+str(year_hist)+'.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937e9183-7997-4794-9ec1-3118954e41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出一下\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b47a403a-9b3c-4e0e-b991-369bd4729f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "【20250106】\n",
    "# 1、重新修改了start_point<harvest_point的存在一小部分问题，主要是before数据替换的不太正确\n",
    "# 2、添加了start_point>harvest_point，即跨年的情况\n",
    "\n",
    "【20250107】\n",
    "# 1、重新修正了跨年的问题，跨年，前面的产量（开始到16周）应该是去年的，而非当年的\n",
    "'''\n",
    "\n",
    "'''\n",
    "【20250106】\n",
    "# 1、重新修改了start_point<harvest_point的存在一小部分问题，主要是before数据替换的不太正确\n",
    "# 2、添加了start_point>harvest_point，即跨年的情况\n",
    "\n",
    "【20250107】\n",
    "# 1、重新修正了跨年的问题，跨年，前面的产量（开始到16周）应该是去年的，而非当年的\n",
    "\n",
    "\n",
    "【20250316】\n",
    "\n",
    "修正历史数据的错误，按照巴基斯坦修正后的代码 目前 美国 巴基斯坦 欧洲 阿根廷 澳大利亚 加拿大 印度已经修正 双区域俄罗斯和美国\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "for region in regions:\n",
    "        # 读取筛选的变量，用于后续变量筛选\n",
    "        Forecastyear = Forecastyears[region]\n",
    "        SelFeature_infornamtion = extract_selected_variables(inputpath_base)\n",
    "        TimeFeatures_sel, Static_sel, regionID = SelFeature_infornamtion[SelFeature_infornamtion['regionID'] == region].iloc[0]\n",
    "        # 实际建模的周数\n",
    "        inpath_dates = os.path.join(inputpath_base, '01_data','05_buildmodel', '02_extractdates','gs_three_periods.txt')\n",
    "        gs_infornamtion = pd.read_csv(inpath_dates, delim_whitespace=True, header=None)\n",
    "        gs_infornamtion.columns = ['start_point', 'peak', 'harvest_point', 'VI_select2','regionID']\n",
    "        start_point, peak, harvest_point, VI_select2, region = gs_infornamtion[gs_infornamtion['regionID'] == region].iloc[0]\n",
    "        print(harvest_point)\n",
    "        # 数据读取和指数的筛选\n",
    "        data_ori_all = pd.read_csv(os.path.join(inputpath_base, '01_data','05_buildmodel','01_weekdata',region+'_allweekYielddata_VIs.csv'))\n",
    "        data_ori_all = data_ori_all.drop_duplicates(subset=['year', 'idJoin'],keep='last')\n",
    "        Static_sel= [col for col in Static_sel if 'year.1' not in col] \n",
    "        TimeFeatures_sel_all= [col for col in data_ori_all.columns if any(feature in col for feature in TimeFeatures_sel)]\n",
    "        TimeFeatures_sel_all= [col for col in TimeFeatures_sel_all if 'Previous_Yield' not in col] # 注意前一年的产量会因为pre降雨而被筛选到，仔细确认\n",
    "        filtered_columns_all = TimeFeatures_sel_all+Static_sel\n",
    "        data_ori_all = data_ori_all[filtered_columns_all+['idJoin','Yield']] # 筛选选择的变量进入后续分析\n",
    "    \n",
    "        \n",
    "        # 筛选VI进行后续的识别\n",
    "        filtered_columns_VI = [col for col in data_ori_all.columns if VI_select2 in col]\n",
    "        data_S2S_VI = data_ori_all[filtered_columns_VI + ['year','idJoin']]\n",
    "        data_S2S_VI_mean = data_S2S_VI[filtered_columns_VI + ['year']].groupby('year').mean()\n",
    "        if start_point < harvest_point: # 同年生长\n",
    "            hisWeekList = ['leadweek_'+str(week) for week in range(1,harvest_point-start_point+1)] # 设定的hisWeekList好像不包括了start_point周\n",
    "        else:\n",
    "            hisWeekList = ['leadweek_'+str(week) for week in range(1,harvest_point-start_point+1+46)]+['leadweek_'+str(week) for week in range(1,harvest_point-start_point+1)]\n",
    "        hist_inputpath = os.path.join(inputpath_base,'02_S2S','03_outputData','02_histdata',region)\n",
    "        data_ori_current = data_ori_all[data_ori_all['year']==Forecastyear]\n",
    "        hist_start_year = Forecastyear-30;hist_end_year = Forecastyear-1;\n",
    "    \n",
    "        for year_hist in range(hist_start_year,hist_end_year+1):\n",
    "            # 处理当前历史年的数据\n",
    "            data_his_new_ori = pd.read_csv(os.path.join(hist_inputpath,'hist_'+str(year_hist)+'.csv'))\n",
    "            data_his_new_ori = data_his_new_ori.drop_duplicates(subset=['idJoin'],keep='last')\n",
    "            data_his_new_ori.set_index('idJoin', inplace=True)\n",
    "            data_his_new_ori['year'] = Forecastyear\n",
    "            data_ori_all = data_ori_all.drop_duplicates(subset=['year', 'idJoin'],keep='last')\n",
    "            data_his_new = data_his_new_ori.copy()\n",
    "            data_his_new = process_climate_data(data_his_new.reset_index(), Forecastyear, T_upper, T_lower, dynamic_features)\n",
    "            data_his_new = data_his_new.dropna(how='all',axis=1) # process_climate_data会引入全部的植被指数\n",
    "            # 将data_his_new计算的有误差（缺失值并未处理），更换成之前建模插补过得\n",
    "            hist_outputpath1 = os.path.join(inputpath_base,'02_S2S','05_WeekData','02_hist',region)\n",
    "            os.makedirs(hist_outputpath1,exist_ok=True)\n",
    "            data_his_new.index=data_his_new_ori.index\n",
    "            data_his_new.to_csv(os.path.join(hist_outputpath1,'hist_'+str(year_hist)+'.csv'))\n",
    "            \n",
    "            data_his_new_update = data_ori_current.copy()\n",
    "            #data_his_new = data_his_new.merge(data_ori_current[filtered_columns_VI+Static_sel+['idJoin','Yield']],on='idJoin',how='inner')# 将VI，静态变量和Y update上面去，数据种类预期保持一致\n",
    "            data_his_new['year'] = Forecastyear \n",
    "\n",
    "\n",
    "            for ii in hisWeekList:\n",
    "                # data_his_new = data_his_new[filtered_columns_all+['idJoin']]\n",
    "                ############################################## 找最相似的植被指数填充，不跨年， start_point<harvest_point####\n",
    "                if int(ii[9:])>harvest_point: # 这里只需要区分week_forecast预报周是否跨年\n",
    "                    week_forecast = harvest_point+1-int(ii[9:])+46\n",
    "                else:\n",
    "                    week_forecast = harvest_point+1-int(ii[9:])\n",
    "\n",
    "                if start_point < harvest_point:  \n",
    "                    # 同一年种植和收获\n",
    "                    forecast_weeklist1 = range(week_forecast, harvest_point + 1)\n",
    "                    V1= [f'Week{week}{VI_select2}' for week in range(1, week_forecast)]; # before的植被指数\n",
    "                    V2= [f'Week{week}{VI_select2}' for week in forecast_weeklist1];# 寻找要被要被预报的# 预报当前周到收获的日子week_forecast是没有的\n",
    "            \n",
    "                    current_S2S_VI_before =data_S2S_VI_mean.loc[Forecastyear][V1]\n",
    "                    dtw_distances = {}\n",
    "                    for year1 in range(startyear,Forecastyear):# 不会取到开始年到Forecastyear前一年\n",
    "                        other_S2S_VI_before = data_S2S_VI_mean.loc[year1][V1]\n",
    "                        distance, path = fastdtw(current_S2S_VI_before, other_S2S_VI_before)# 预报当前周到收获的日子\n",
    "                        dtw_distances[year1] = distance\n",
    "                    most_similar_by_dtw = min(dtw_distances, key=dtw_distances.get) # \n",
    "                    data_S2S_VI_forecast2 = data_S2S_VI[data_S2S_VI['year'] == most_similar_by_dtw][V2+['idJoin']]# 只需要建模的数据\n",
    "                    data_his_new_update = data_his_new_update.drop(V2,axis=1) # 删除原有的预报日期对应的，不预报还是保留\n",
    "                    data_his_new_update = data_his_new_update.merge(data_S2S_VI_forecast2,on='idJoin',how='inner')\n",
    "                else:\n",
    "                    ############################################## 找最相似的植被指数填充，跨年， start_point>harvest_point####\n",
    "                    week_forecast = harvest_point+1-int(ii[9:]) # \n",
    "                    if week_forecast<=0: # 预报期市前一年，计算的为负数；就要加上46\n",
    "                        week_forecast = harvest_point+1-int(ii[9:])+46 # \n",
    "                    else:\n",
    "                        week_forecast = week_forecast\n",
    "\n",
    "                    if week_forecast<=harvest_point: # = 是当前的第一周\n",
    "                        # 上全年，因为跨年很有可能在第一周，这样必须要前一年去找相似年；与同一年生殖期不一样的是，一般同一年生殖期前面会有一个序列\n",
    "                        forecast_weeklist1 = range(week_forecast, harvest_point+1)\n",
    "                        V1_1 = [f'Week{week}{VI_select2}' for week in range(1, 46+1)];\n",
    "                        V1_2 = [f'Week{week}{VI_select2}' for week in range(1, week_forecast)];\n",
    "                        V2 = [f'Week{week}{VI_select2}' for week in forecast_weeklist1];\n",
    "                        current_S2S_VI_before =pd.concat([data_S2S_VI_mean.loc[Forecastyear][V1_1], data_S2S_VI_mean.loc[Forecastyear-1][V1_2]])\n",
    "                        dtw_distances = {}\n",
    "\n",
    "                        for year1 in range(startyear+1,Forecastyear):# 需要两年，不会取到Forecastyear年\n",
    "                            other_S2S_VI_before = pd.concat([data_S2S_VI_mean.loc[year1][V1_1], data_S2S_VI_mean.loc[year1-1][V1_2]])\n",
    "                            distance, path = fastdtw(current_S2S_VI_before, other_S2S_VI_before)# 预报当前周到收获的日子\n",
    "                            dtw_distances[year1] = distance\n",
    "\n",
    "                        most_similar_by_dtw = min(dtw_distances, key=dtw_distances.get) \n",
    "\n",
    "                        # 只需要当前年替换建模的数据\n",
    "\n",
    "                        data_S2S_VI_forecast2 = data_S2S_VI[data_S2S_VI['year'] == most_similar_by_dtw][V2+['idJoin']]\n",
    "                        data_his_new_update = data_his_new_update.drop(V2,axis=1) # 删除原有的预报日期对应的，不预报还是保留\n",
    "                        data_his_new_update = data_his_new_update.merge(data_S2S_VI_forecast2,on='idJoin',how='inner')\n",
    "\n",
    "                    else:  \n",
    "                        # 跨年，替换的就是 list(range(week_forecast, 46))+list(range(1,harvest_point + 1))\n",
    "\n",
    "                        forecast_weeklist1 = list(range(week_forecast, 46+1))+list(range(1,harvest_point + 1))\n",
    "                        \n",
    "                        V1_1= [f'Week{week}{VI_select2}' for week in range(1, week_forecast)]; # 前一年\n",
    "                        V2_1 =  [f'Week{week}{VI_select2}' for week in range(week_forecast, 46+1)]; # 前一年\n",
    "                        V2_2 =  [f'Week{week}{VI_select2}' for week in range(1,harvest_point + 1)]; # 当年\n",
    "\n",
    "                        current_S2S_VI_before =data_S2S_VI_mean.loc[Forecastyear-1][V1_1]\n",
    "                        dtw_distances = {}\n",
    "                        for year1 in range(startyear+1,Forecastyear-1):# 不会取到Forecastyear年\n",
    "                            other_S2S_VI_before = data_S2S_VI_mean.loc[year1-1][V1_1]\n",
    "                            distance, path = fastdtw(current_S2S_VI_before, other_S2S_VI_before)# 预报当前周到收获的日子\n",
    "                            dtw_distances[year1] = distance\n",
    "                        most_similar_by_dtw = min(dtw_distances, key=dtw_distances.get) # 找到2016年\n",
    "                            \n",
    "                        data_S2S_VI_forecast1 = data_S2S_VI[data_S2S_VI['year'] == most_similar_by_dtw][V2_1+['idJoin']]# 只需要建模的数据\n",
    "                        data_S2S_VI_forecast2 = data_S2S_VI[data_S2S_VI['year'] == most_similar_by_dtw+1][V2_2]# 只需要建模的数据\n",
    "                        data_S2S_VI_forecast2 = pd.concat([data_S2S_VI_forecast1.reset_index(drop=True), data_S2S_VI_forecast2.reset_index(drop=True)], axis=1)# 横向拼接\n",
    "                        data_his_new_update = data_his_new_update.drop(V2_1+V2_2,axis=1) # 删除原有的预报日期对应的，不预报还是保留\n",
    "                        data_his_new_update = data_his_new_update.merge(data_S2S_VI_forecast2,on='idJoin',how='inner')               \n",
    "                data_his_new_update.set_index('idJoin', inplace=True)\n",
    "                ############################################## hist替换原始数据的需要预报的周数 ###################################################################\n",
    "                if week_forecast<=harvest_point: # 说明不跨年;只需要替换用到当年的数据\n",
    "                    update_climate = []\n",
    "                    for feature in [feature for feature in TimeFeatures_sel if feature != VI_select2[1:]]: # 除了植被指数的所选气象数据\n",
    "                        update_climate += [f'Week{week}_{feature}' for week in forecast_weeklist1] # 虽然有，但是你要记住啊，是存在跨年的，意味着要 \n",
    "                        \n",
    "                    data_his_new_update[update_climate] = data_his_new[update_climate] # 替换是his来替换原始，\n",
    "                else: # 跨年需要用到前一年的历史数据，\n",
    "                    # 读取处理前一年的数据，week_forecast到46的替换数据\n",
    "                    data_his_new_ori_lastyear = pd.read_csv(os.path.join(hist_inputpath,'hist_'+str(year_hist-1)+'.csv'))\n",
    "                    data_his_new_ori_lastyear.set_index('idJoin', inplace=True)\n",
    "                    data_his_new_ori_lastyear['year'] = Forecastyear\n",
    "                    data_his_new_lastyear = data_his_new_ori_lastyear.copy()\n",
    "                    data_his_new_lastyear = process_calimate_data(data_his_new_lastyear.reset_index(), Forecastyear, T_upper, T_lower, dynamic_features)\n",
    "                    data_his_new_lastyear = data_his_new_lastyear.dropna(how='all',axis=1) # process_climate_data会引入全部的植被指数\n",
    "\n",
    "                    # 前一年range(week_forecast, 46)\n",
    "                    update_climate1 = []\n",
    "                    for feature in [feature for feature in TimeFeatures_sel if feature != VI_select2[1:]]: # 除了植被指数的所选气象数据\n",
    "                        update_climate1 += [f'Week{week}_{feature}' for week in list(range(week_forecast, 46))] # \n",
    "                    data_his_new_update[update_climate1] = data_his_new_lastyear[update_climate1] # 替换是his来替换原始，\n",
    "                    \n",
    "                    # 当年的1到harvest_point\n",
    "                    update_climate2 = []\n",
    "                    for feature in [feature for feature in TimeFeatures_sel if feature != VI_select2[1:]]: # 除了植被指数的所选气象数据\n",
    "                        update_climate2 += [f'Week{week}_{feature}' for week in list(range(1, harvest_point + 1))] # \n",
    "                    data_his_new_update[update_climate2] = data_his_new[update_climate2] # 替换是his来替换原始，\n",
    "                ############################################## 筛选生育期的变量 ############################################################################\n",
    "                data_his_new_update = data_his_new_update.reset_index()\n",
    "                weeks = []\n",
    "                # 判断是否跨年\n",
    "                if start_point < harvest_point:  # 不跨年\n",
    "                    for feature in TimeFeatures_sel:\n",
    "                        # 使用列表生成器生成周和特征的组合\n",
    "                        weeks += [f'Week{week}_{feature}' for week in range(start_point, harvest_point + 1)]\n",
    "                    gs_features = weeks + Static_sel+['Yield']+['idJoin']\n",
    "                    data_his_new_update = data_his_new_update[gs_features]\n",
    "                else:  # 跨年\n",
    "                    for feature in TimeFeatures_sel:\n",
    "                        # 合并两段范围并生成周和特征的组合\n",
    "                        weeks += [f'Week{week}_{feature}' for week in list(range(start_point, 47)) + list(range(1, harvest_point + 1))]\n",
    "                     # 跨年前面的产量应该是替换成前一年的，_data_ori是对的；但是新生成的还是当年的\n",
    "                    data= pd.read_csv(os.path.join(inputpath_base, '01_data','05_buildmodel','03_modeldata',region+'_data_ori.csv'))\n",
    "                    data = data.drop_duplicates(subset=['year', 'idJoin'],keep='last')\n",
    "                    weeks = []\n",
    "                    for feature in TimeFeatures_sel:\n",
    "                        weeks += [f'Week{week}_{feature}' for week in list(range(start_point, 47))]     \n",
    "                    data = data[weeks+['idJoin','year']];\n",
    "                    data_his_new_update = data_his_new_update.drop(weeks,axis=1);\n",
    "                    data = data[data['year']==Forecastyear]\n",
    "                    data_his_new_update = data_his_new_update.merge(data,on=['idJoin','year'],how='inner')  \n",
    "                # 替换前一年的\n",
    "                ##############################################输出 ############################################################################\n",
    "                hist_outputpath = os.path.join(inputpath_base,'02_S2S','06_buildmodel','02_hist','VI_Like',region,ii)\n",
    "                os.makedirs(hist_outputpath,exist_ok=True)\n",
    "                data_his_new_update.to_csv(os.path.join(hist_outputpath,'hist_'+str(year_hist)+'.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde96cce-e477-4430-99dd-3146a4420b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0064f3-1ce5-451c-acaa-50702b68975c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2c9b778-e41a-4777-bd1c-7e4c6e0c83b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce458e-9806-4865-9ed3-c28345f92eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ad3c0-2c4a-4d82-94fa-da637c414597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203a06d-0917-440c-946c-e75986fda1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba673555-31ce-4cc2-9002-c5448eaaf852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c5169-c733-48c3-b673-4fc2161c6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94585720-19e0-45ae-bcf4-8539429cf358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62368e-22fc-4f81-9b2e-15179058b719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c933b04-9806-4d50-a05e-cd663d5d66fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146e081-2d77-428a-b40d-d97de589cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba74bdd-ca6e-449e-8ac6-95fb0e8b85f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854d544-77ab-4545-bec8-b9de8eab6971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e65841-9034-4161-ac6b-df6136b17d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c702591-98d9-421c-8c1b-92c6bb1cf43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ae1ce-e7a4-4ec2-a4e0-b20adfaa8a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591477e-c876-43c9-8622-17777cb087ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "\n",
    "# 找到相似年份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b2b1c-a66a-46af-b284-27bcef98f432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44fb920-1fa1-431c-8dd6-2f661f878475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# data_S2S_new_all_new['year'] == current_S2S_VI['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4919cc18-a20f-4d18-9e77-8459362eb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5fbdf-f966-421b-aa7e-bfa743dd031d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0896bb-69ca-4f8e-8585-086973b0f84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9928f8-9c37-481a-9af9-56547be9a6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1102e-c46e-43aa-b8c6-9f7949cdc5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f0abb-1754-4c40-956b-b8bf2b1bab05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9160d-1c84-4e9f-93c9-d7889cc3b885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f3b2a5-4e6c-45c4-9117-a3f59b7ab74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d1b2f-1d18-43d7-8f19-7ac6a4629654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e94185-ebff-4c36-94d5-007a1847f77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f27b67-9c7c-406f-b75f-a486f9a450df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066a011-752e-4a6f-b208-5c193228ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935feaa-c37e-411e-a38f-665e010a9bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b30446-e6b4-4775-afb9-e51d6d47dbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce59ee6-74a2-4702-bf43-679cf8490f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d2371-ddde-43b0-9562-8ea98ce2dd82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b000c49-389f-4a4c-bbac-291af0deecc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
